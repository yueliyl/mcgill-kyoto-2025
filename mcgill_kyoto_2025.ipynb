{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yueliyl/mcgill-kyoto-2025/blob/main/mcgill_kyoto_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsupervised learning of single-cell transcriptome data using NMF and ETM\n",
        "\n",
        "In this assignment, you will directly modify the code blocks as instructed below to implement and experiment non-negative matrix factorization (NMF) and embedded topic model (ETM) on a single-cell RNA-seq datasets.\n",
        "\n",
        "For the rest of the codeblocks, simply click through them **without making any change**. You need to read and understand each codeblock. But we will only grade the assigned code blocks and the desired outputs.\n",
        "\n",
        "You are recommended to run the code using Colab. By the default Colab connects to a CPU node. To speed up the training, you may connect with a T4 GPU by choosing 'Runtime' in the top menu and then clicking on 'Change runtime type' and choose 'T4 GPU'. Note that for free Google account, we can keep the notebook connected to a GPU for at most 12 hours. Nonetheless, all the training and inference required to complete this assignment can be done within 5 minutes even with a CPU.\n",
        "\n",
        "If you have a good GPU-equipped Windows or Mac computer, you may connect to Local Runtime:\n",
        "\n",
        "1. Install Jupyter Package for Colab: in Terminal with bash shell, do\n",
        "\n",
        "  ```\n",
        "  pip install jupyter_http_over_ws\n",
        "  jupyter serverextension enable --py jupyter_http_over_ws\n",
        "  ```\n",
        "  \n",
        "2. In Terminal, start a Jupyter Notebook instance on your local machine:\n",
        "```\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --no-browser\n",
        "```\n",
        "Copy one of the URLs generated as a token. From my side, it looks like this `http://localhost:8888/?token=[long string]`\n",
        "\n",
        "3. Connect to Local Runtime in Colab:\n",
        "  1. Open your Colab notebook and select Connect â†’ Connect to local runtime.\n",
        "  2. When prompted with local URL, paste the token URL you got from step 2.\n",
        "\n",
        "After this setup, Colab will connect to the local Jupyter Notebook server, allowing you to utilize your local machine's resources while running the notebook on Google Colab's interface.\n",
        "\n",
        "In my experiments, I used a Macbook Pro with M1 Max and 64 GB RAM. The speed is comparable to the T4 GPU and much faster than the default CPU on Colab.\n"
      ],
      "metadata": {
        "id": "aMeS2qjglbXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install scanpy for single-cell data analysis\n",
        "!pip install scanpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UaaOm6dD7WW",
        "outputId": "4e2ae91f-0ec2-4619-c872-42ad633e545e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scanpy in /Users/yueli/anaconda3/lib/python3.11/site-packages (1.9.5)\r\n",
            "Requirement already satisfied: anndata>=0.7.4 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (0.10.2)\r\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (1.24.3)\r\n",
            "Requirement already satisfied: matplotlib>=3.4 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (3.7.2)\r\n",
            "Requirement already satisfied: pandas>=1.0 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (2.0.3)\r\n",
            "Requirement already satisfied: scipy>=1.4 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (1.11.1)\r\n",
            "Requirement already satisfied: seaborn in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (0.12.2)\r\n",
            "Requirement already satisfied: h5py>=3 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (3.9.0)\r\n",
            "Requirement already satisfied: tqdm in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (1.3.0)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (0.14.0)\n",
            "Requirement already satisfied: patsy in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: networkx>=2.3 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (3.1)\n",
            "Requirement already satisfied: natsort in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: joblib in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (1.2.0)\n",
            "Requirement already satisfied: numba>=0.41.0 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (0.57.1)\n",
            "Requirement already satisfied: umap-learn>=0.3.10 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (0.5.4)\n",
            "Requirement already satisfied: packaging in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (23.1)\n",
            "Requirement already satisfied: session-info in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scanpy) (1.0.0)\n",
            "Requirement already satisfied: array-api-compat in /Users/yueli/anaconda3/lib/python3.11/site-packages (from anndata>=0.7.4->scanpy) (1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4->scanpy) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4->scanpy) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4->scanpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from numba>=0.41.0->scanpy) (0.40.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from pandas>=1.0->scanpy) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from pandas>=1.0->scanpy) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.24->scanpy) (2.2.0)\n",
            "Requirement already satisfied: six in /Users/yueli/anaconda3/lib/python3.11/site-packages (from patsy->scanpy) (1.16.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from umap-learn>=0.3.10->scanpy) (0.5.10)\n",
            "Requirement already satisfied: stdlib-list in /Users/yueli/anaconda3/lib/python3.11/site-packages (from session-info->scanpy) (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install igraph and louvain needed for clustering evaluation below\n",
        "!pip install python-igraph\n",
        "!pip install louvain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gPGX-J7NTCn",
        "outputId": "a584d62a-f264-4911-c4bb-c1341f13f198"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-igraph in /Users/yueli/anaconda3/lib/python3.11/site-packages (0.11.6)\r\n",
            "Requirement already satisfied: igraph==0.11.6 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from python-igraph) (0.11.6)\r\n",
            "Requirement already satisfied: texttable>=1.6.2 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from igraph==0.11.6->python-igraph) (1.7.0)\n",
            "Requirement already satisfied: louvain in /Users/yueli/anaconda3/lib/python3.11/site-packages (0.8.2)\n",
            "Requirement already satisfied: igraph<0.12,>=0.10.0 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from louvain) (0.11.6)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /Users/yueli/anaconda3/lib/python3.11/site-packages (from igraph<0.12,>=0.10.0->louvain) (1.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iC6vssu2lVCQ"
      },
      "outputs": [],
      "source": [
        "#@title Import all required libraries for this assignment\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import scanpy as sc\n",
        "import anndata\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.patches import Patch\n",
        "from seaborn import heatmap, lineplot, clustermap\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import igraph as ig\n",
        "import louvain as lv\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "random.seed(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you running locally and get ModuleNotFoundError when importing any of the above packages, do pip install them into your system."
      ],
      "metadata": {
        "id": "iWjpBYSW1qic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mouse Pancreas single-cell RNA-sequencing data\n",
        "The folder a4_data contains the single-cell Mouse Pancreas dataset obtained from Baron et al (2016) study. The dataset has 1886 cells measured over 4194 genes by scRNA-seq and were processed and filtered for the purpose of this assignment. Here are the individual files:\n",
        "\n",
        "* `MP.pickle`: The single-cell expression count matrix (matrix of integers) of dimension 1886 cells by 4194 genes plus 2 columns indicating the batch IDs and cell types\n",
        "* `MP_genes.pickle`: gene names for the 4194 genes\n",
        "* `sample_info.csv`: a 1886 by 3 information matrix with the rows as the1886 cells and columns for the cell IDs, batch IDs (not used in this assignment), and cell type labels\n",
        "* `cell_IDs.pkl`: cell IDs as a list of strings\n",
        "\n",
        "The code block below loads the scRNA-seq data into a Pandas DataFrame `df` as well as the M gene names `genes` using a Python library called `pickle`. It then reorders the rows and columns and save the final $N\\times M$ matrix $X$ into a Numpy `ndarray` for our subsequent matrix factorization tasks.\n",
        "\n",
        "In addition, using `anndata` library we create an AnnData object called `mp_anndata`, which as the input ndarray matrix $X$ and the ground-truth cell type labels for the 1886 cells for evaluation purpose."
      ],
      "metadata": {
        "id": "MXE7iGXnl9S4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nqSVSNIzlVCR"
      },
      "outputs": [],
      "source": [
        "# If run on Colab cloud compute (default), uncomment this code\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # change this to the path where the data are saved on your Google Drive\n",
        "# !git clone https://github.com/yueliyl/mcgill-kyoto-2025/content/drive/MyDrive/mcgill-kyoto-2025\n",
        "# filepath = \"/content/drive/MyDrive/mcgill-kyoto-2025/\"\n",
        "\n",
        "# If running on local runtime, uncomment this change the file path if needed\n",
        "filepath = \"~/Documents/Talks/McGill-Kyoto-2025/tut/\"\n",
        "filepath = os.path.expanduser(filepath)\n",
        "\n",
        "# Extract to the current directory\n",
        "import zipfile\n",
        "with zipfile.ZipFile(filepath+'MP.pickle.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(filepath)\n",
        "\n",
        "# mouse pancreas single-cell dataset\n",
        "# read in data and cell type labels\n",
        "with open(filepath+'MP.pickle', 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "with open(filepath+'MP_genes.pickle', 'rb') as f:\n",
        "    genes = pickle.load(f)\n",
        "\n",
        "df.set_index('Unnamed: 0', inplace=True)  # set first column (cell ID as the index column)\n",
        "sample_id = pickle.load(open(filepath+'cell_IDs.pkl', 'rb'))\n",
        "df = df.loc[list(sample_id), :]\n",
        "\n",
        "X = df[genes].values  # extract the N x M cells-by-genes matrix\n",
        "\n",
        "sample_info = pd.read_csv(filepath+'sample_info.csv')\n",
        "\n",
        "mp_anndata = anndata.AnnData(X=X)\n",
        "\n",
        "mp_anndata.obs['Celltype'] = sample_info['assigned_cluster'].values\n",
        "\n",
        "N = X.shape[0]  # number of single-cell samples\n",
        "K = 16  # number of topics\n",
        "M = X.shape[1]  # number of genes\n",
        "\n",
        "X_tensor = torch.from_numpy(np.array(X, dtype=\"float32\"))\n",
        "sums = X_tensor.sum(1).unsqueeze(1)\n",
        "X_tensor_normalized = X_tensor / sums"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating clustering by adjusted Rand Index\n",
        "Adjusted Rand Index (ARI) is a common metric used to evaluate the unsupervised clustering by comparing the consistency between the predicted clusters and the ground-truth clusters (i.e., cell type labels in this assignment).\n",
        "\n",
        "The function `evaluate_ari(cell_embed, adata)` below takes $N\\times K$ input cell embedding `cell_embed` with $K$ as the embedding dimensions and the annotated cell label data as `adata`. It first runs UMAP to compute the distance between cells based on their embeddings and then run Louvain clustering using the cells-cells distance matrix from UMAP to cluster cells into groups defined by the resolution parameter (default: 0.15). Finally, it computes the ARI based on the Louvain clusters and the ground-truth cell type using `adjusted_rand_score` from `Scikit-learn`.\n",
        "\n",
        "We will be using `evaluate_ari` to evaluate the cell embedding quality by NMF and ETM."
      ],
      "metadata": {
        "id": "eybR6Ll8KkGa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "22z7IQ6TlVCS"
      },
      "outputs": [],
      "source": [
        "def evaluate_ari(cell_embed, adata):\n",
        "    \"\"\"\n",
        "        This function is used to evaluate ARI using the lower-dimensional embedding\n",
        "        cell_embed of the single-cell data\n",
        "        :param cell_embed: a NxK single-cell embedding generated from NMF or scETM\n",
        "        :param adata: single-cell AnnData data object (default to to mp_anndata)\n",
        "        :return: ARI score of the clustering results produced by Louvain\n",
        "    \"\"\"\n",
        "    adata.obsm['cell_embed'] = cell_embed\n",
        "    sc.pp.neighbors(adata, use_rep=\"cell_embed\", n_neighbors=30)\n",
        "    sc.tl.louvain(adata, resolution=0.15)\n",
        "    ari = adjusted_rand_score(adata.obs['Celltype'], adata.obs['louvain'])\n",
        "    return ari"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 NMF with SSE loss\n",
        "\n",
        "Implement the NMF function `nmf_sse` based on Lecture 13 slide 19. A function header, Docstring, and function call have been written for you.\n",
        "\n",
        "Your implementation can be in either Numpy or PyTorch. The latter is much faster when running on a GPU.\n",
        "\n",
        "Note that the NMF takes an input matrix of dimension $M\\times N$ with $M$ genes and $N$ samples whereas the input matrix $X$ is a $N\\times M$ dimension. Also, the matrix $H$ is a $K\\times N$ matrix where the `evaluate_ari(cell_embed, adata)` above expects the cell embedding to have dimension $N\\times K$. So you will need to provide these functions with the transposed of $X$ and $H$, respectively.\n",
        "\n",
        "Besides returning the final matrices $W$ and $H$, save the mean squared error (MSE) $MSE = ||X-WH||_2^2/(N*M)$ and ARI at each iteration into a 3-column ndarray called `perf` with the first column as the iteration index and return `perf` as the third output from the function `nmf_sse`.\n",
        "\n",
        "Run your NMF for 100 iterations and return $W$, $H$, and `perf` for the following analyses. This should take less than 3 minutes depending on your local computer. This should takes less than a minute to finish."
      ],
      "metadata": {
        "id": "LIiKHkWrmMHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NMF with SSE loss function\n",
        "def nmf_sse(X, K, adata=mp_anndata, niter=100):\n",
        "    \"\"\"\n",
        "    NMF with sum of squared error loss as the objective\n",
        "    :param X: M x N input matrix (numpy array)\n",
        "    :param K: low rank\n",
        "    :param adata: annotated X matrix with cluster labels for evaluating ARI\n",
        "    :param niter: number of iterations to run\n",
        "    :return:\n",
        "        1. updated W and H that minimize sum of squared error ||X - WH||^2_F s.t. W,H>=0\n",
        "        2. niter-by-3 tensor with iteration index, SSE, and ARI as the 3 columns\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize W and H with random values, ensuring they're on the GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    M, N = X.shape\n",
        "    W = torch.rand(M, K, device=device)\n",
        "    H = torch.rand(K, N, device=device)\n",
        "    X = torch.from_numpy(np.array(X, dtype=\"float32\"))\n",
        "\n",
        "    # Ensure X, W, and H are on the correct device\n",
        "    X = X.to(device)\n",
        "    W = W.to(device)\n",
        "    H = H.to(device)\n",
        "\n",
        "    # Initialize performance tracking array\n",
        "    perf = torch.zeros((niter, 3), dtype=torch.float32, device=device)\n",
        "\n",
        "    for i in range(niter):\n",
        "\n",
        "        # Update H and W with element-wise multiplication and division\n",
        "        H = H * (W.T @ X) / (W.T @ W @ H + 1e-10)  # Adding a small constant to avoid division by zero\n",
        "        W = W * (X @ H.T) / (W @ (H @ H.T) + 1e-10)\n",
        "\n",
        "        # Calculate the performance metrics\n",
        "        reconstruction = W @ H\n",
        "        sse = torch.sum((reconstruction - X) ** 2) / (X.size(0) * X.size(1))\n",
        "        H_np = H.T.cpu().numpy() if H.is_cuda else H.T.numpy()\n",
        "        ari = evaluate_ari(H_np, adata)\n",
        "\n",
        "        # Store iteration, SSE, and ARI\n",
        "        perf[i, 0] = i\n",
        "        perf[i, 1] = sse\n",
        "        perf[i, 2] = ari\n",
        "\n",
        "        print(f\"Iter: {i} .. MSE: {sse:.4f} .. ARI: {ari:.4f}\")\n",
        "\n",
        "    return W.cpu().numpy(), H.cpu().numpy(), perf.cpu()"
      ],
      "metadata": {
        "id": "44dBI-1API1K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_nmf_sse, H_nmf_sse, nmf_sse_perf = nmf_sse(X.T, K, niter=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4fd3Dy5tRD4Y",
        "outputId": "314afdec-dff2-45a8-9dc6-d2e5624d0768"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 1 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 2 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 3 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 4 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 5 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 6 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 7 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 8 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 9 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 10 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 11 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 12 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 13 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 14 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 15 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 16 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 17 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 18 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 19 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 20 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 21 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 22 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 23 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 24 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 25 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 26 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 27 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 28 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 29 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 30 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 31 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 32 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 33 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 34 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 35 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 36 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 37 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 38 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 39 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 40 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 41 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 42 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 43 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 44 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 45 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 46 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 47 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 48 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 49 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 50 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 51 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 52 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 53 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 54 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 55 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 56 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 57 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 58 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 59 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 60 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 61 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 62 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 63 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 64 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 65 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 66 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 67 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 68 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 69 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 70 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 71 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 72 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 73 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 74 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 75 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 76 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 77 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 78 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 79 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 80 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 81 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 82 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 83 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 84 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 85 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 86 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 87 .. MSE: 62.9560 .. ARI: 0.0000\n",
            "Iter: 88 .. MSE: 62.9560 .. ARI: 0.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m W_nmf_sse, H_nmf_sse, nmf_sse_perf \u001b[38;5;241m=\u001b[39m nmf_sse(X\u001b[38;5;241m.\u001b[39mT, K, niter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mnmf_sse\u001b[0;34m(X, K, adata, niter)\u001b[0m\n\u001b[1;32m     33\u001b[0m sse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((reconstruction \u001b[38;5;241m-\u001b[39m X) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     34\u001b[0m H_np \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m H\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;28;01melse\u001b[39;00m H\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 35\u001b[0m ari \u001b[38;5;241m=\u001b[39m evaluate_ari(H_np, adata)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Store iteration, SSE, and ARI\u001b[39;00m\n\u001b[1;32m     38\u001b[0m perf[i, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n",
            "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mevaluate_ari\u001b[0;34m(cell_embed, adata)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    This function is used to evaluate ARI using the lower-dimensional embedding\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    cell_embed of the single-cell data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    :return: ARI score of the clustering results produced by Louvain\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m adata\u001b[38;5;241m.\u001b[39mobsm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_embed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cell_embed\n\u001b[0;32m---> 10\u001b[0m sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mneighbors(adata, use_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     11\u001b[0m sc\u001b[38;5;241m.\u001b[39mtl\u001b[38;5;241m.\u001b[39mlouvain(adata, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m)\n\u001b[1;32m     12\u001b[0m ari \u001b[38;5;241m=\u001b[39m adjusted_rand_score(adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCelltype\u001b[39m\u001b[38;5;124m'\u001b[39m], adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlouvain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scanpy/neighbors/__init__.py:148\u001b[0m, in \u001b[0;36mneighbors\u001b[0;34m(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m     adata\u001b[38;5;241m.\u001b[39m_init_as_actual(adata\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    147\u001b[0m neighbors \u001b[38;5;241m=\u001b[39m Neighbors(adata)\n\u001b[0;32m--> 148\u001b[0m neighbors\u001b[38;5;241m.\u001b[39mcompute_neighbors(\n\u001b[1;32m    149\u001b[0m     n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors,\n\u001b[1;32m    150\u001b[0m     knn\u001b[38;5;241m=\u001b[39mknn,\n\u001b[1;32m    151\u001b[0m     n_pcs\u001b[38;5;241m=\u001b[39mn_pcs,\n\u001b[1;32m    152\u001b[0m     use_rep\u001b[38;5;241m=\u001b[39muse_rep,\n\u001b[1;32m    153\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    154\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    155\u001b[0m     metric_kwds\u001b[38;5;241m=\u001b[39mmetric_kwds,\n\u001b[1;32m    156\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_added \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     key_added \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighbors\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scanpy/neighbors/__init__.py:785\u001b[0m, in \u001b[0;36mNeighbors.compute_neighbors\u001b[0;34m(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_dense_distances:\n\u001b[1;32m    784\u001b[0m     _distances \u001b[38;5;241m=\u001b[39m pairwise_distances(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetric_kwds)\n\u001b[0;32m--> 785\u001b[0m     knn_indices, knn_distances \u001b[38;5;241m=\u001b[39m _get_indices_distances_from_dense_matrix(\n\u001b[1;32m    786\u001b[0m         _distances, n_neighbors\n\u001b[1;32m    787\u001b[0m     )\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m knn:\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distances \u001b[38;5;241m=\u001b[39m _get_sparse_matrix_from_indices_distances_numpy(\n\u001b[1;32m    790\u001b[0m             knn_indices, knn_distances, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_neighbors\n\u001b[1;32m    791\u001b[0m         )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scanpy/neighbors/__init__.py:468\u001b[0m, in \u001b[0;36m_get_indices_distances_from_dense_matrix\u001b[0;34m(D, n_neighbors)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_indices_distances_from_dense_matrix\u001b[39m(D, n_neighbors: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    467\u001b[0m     sample_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(D\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m--> 468\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(D, n_neighbors \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, :n_neighbors]\n\u001b[1;32m    469\u001b[0m     indices \u001b[38;5;241m=\u001b[39m indices[sample_range, np\u001b[38;5;241m.\u001b[39margsort(D[sample_range, indices])]\n\u001b[1;32m    470\u001b[0m     distances \u001b[38;5;241m=\u001b[39m D[sample_range, indices]\n",
            "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:871\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    794\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    869\u001b[0m \n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margpartition\u001b[39m\u001b[38;5;124m'\u001b[39m, kth, axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monitor training progress\n",
        "THe function called `monitor_perf` below is completed and provided to you. It displays the SSE and ARI at each iteration from the above NMF-SSE training. If your above NMF implementation is correct, you will observe that the SSE drops quite rapidly at the first few iterations and the ARI increases in a zigzag way because it is an independent metric from the training objective.\n",
        "\n",
        "Use plot below as your reference. Although you may see a different progress with different random initialization of $W$ and $H$, the behaviour of your implemented NMF model should be very similar and this is true for all of the following tasks.\n",
        "\n",
        "We are going to use `monitor_perf` to monitor the performances of the other two models that we are going to implement next."
      ],
      "metadata": {
        "id": "wxqQHWzWgvGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Monitor ARI and objective function\n",
        "def monitor_perf(perf, objective, path=\"\"):\n",
        "    \"\"\"\n",
        "    :param perf: niter-by-3 ndarray with iteration index, objective function, and ARI as the 3 columns\n",
        "    :param objective: 'SSE', 'Poisson', or 'NELBO'\n",
        "    :param path: path to save the figure if not display to the screen\n",
        "    :behaviour: display or save a 2-by-1 plot showing the progress of optimizing objective and ARI as\n",
        "        a function of iterations\n",
        "    \"\"\"\n",
        "    perf = pd.DataFrame(data=perf, columns=['Iter', objective, 'ARI'])\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharey=False)\n",
        "    lineplot(data=perf, x=\"Iter\", y=objective, ax=ax1)\n",
        "    lineplot(data=perf, x=\"Iter\", y=\"ARI\", ax=ax2)\n",
        "    if path == \"\":\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(path)\n",
        "\n",
        "monitor_perf(nmf_sse_perf, \"MSE\")"
      ],
      "metadata": {
        "id": "EN1uC4FNf5m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 NMF with log Poisson likelihood\n",
        "Implement the NMF function `nmf_psn` that maximizes the log Poisson likelihood w.r.t. $W$ and $H$ s.t. $W,H\\ge0$ based on Lecture 13 slide 53.\n",
        "\n",
        "Your implementation can be in either Numpy or PyTorch. The latter is much faster when running on a GPU.\n",
        "\n",
        "Save the average log Poisson likelihood $(X\\log WH - WH)/(N\\times M)$ and ARI at each iteration.\n",
        "\n",
        "Note that because the NMF algorithm requires element-wise division, to avoid dividing by zeros, you can set the zero values to a small value like so: `np.where(A > 0, A, 1e-16)`.\n"
      ],
      "metadata": {
        "id": "Jdk5M4oCh488"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NMF Poisson likelihood\n",
        "def nmf_psn(X, K, adata=mp_anndata, niter=100):\n",
        "    \"\"\"\n",
        "    NMF with log Poisson likelihood as the objective\n",
        "    :param X: M x N input matrix\n",
        "    :param K: low rank\n",
        "    :param adata: annotated X matrix with cluster labels for evaluating ARI\n",
        "    :param niter: number of iterations to run\n",
        "    :return:\n",
        "        1. updated W and H that minimize log Poisson likelihood\n",
        "        2. niter-by-3 tensor with iteration index, Loglik, and ARI as the 3 columns\n",
        "    \"\"\"\n",
        "    # Initialize W and H with random values, ensuring they're on the GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    M, N = X.shape\n",
        "    W = torch.rand(M, K, device=device)\n",
        "    H = torch.rand(K, N, device=device)\n",
        "    X = torch.from_numpy(np.array(X, dtype=\"float32\"))\n",
        "\n",
        "    # Ensure X, W, and H are on the correct device\n",
        "    X = X.to(device)\n",
        "    W = W.to(device)\n",
        "    H = H.to(device)\n",
        "\n",
        "    # Initialize performance tracking array\n",
        "    perf = torch.zeros((niter, 3), dtype=torch.float32, device=device)\n",
        "\n",
        "    for i in range(niter):\n",
        "\n",
        "        # Compute intermediate values with small constants to prevent division by zero\n",
        "        WT1 = W.T @ torch.ones((X.size(0), X.size(1)), device=device)\n",
        "        X_hat = W @ H\n",
        "        H = H * (W.T @ (X / (X_hat + 1e-16))) / (WT1 + 1e-16)\n",
        "\n",
        "        oneHT = torch.ones((X.size(0), X.size(1)), device=device) @ H.T\n",
        "        X_hat = W @ H\n",
        "        W = W * ((X / (X_hat + 1e-16)) @ H.T) / (oneHT + 1e-16)\n",
        "\n",
        "        # Ensure X_hat remains positive\n",
        "        X_hat = W @ H\n",
        "        X_hat = torch.where(X_hat > 0, X_hat, torch.tensor(1e-16, device=device))\n",
        "\n",
        "        # Calculate log Poisson likelihood and ARI\n",
        "        log_likelihood = torch.sum(X * torch.log(X_hat) - X_hat) / (X.size(0) * X.size(1))\n",
        "\n",
        "        # Convert H to NumPy for ARI evaluation\n",
        "        H_np = H.T.cpu().numpy() if H.is_cuda else H.T.numpy()\n",
        "        ari = evaluate_ari(H_np, adata) if adata is not None else 0  # Placeholder if no `adata`\n",
        "\n",
        "        # Store iteration, Loglik, and ARI\n",
        "        perf[i, 0] = i\n",
        "        perf[i, 1] = log_likelihood\n",
        "        perf[i, 2] = ari\n",
        "\n",
        "        print(f\"Iter: {i} .. Loglik: {log_likelihood:.4f} .. ARI: {ari:.4f}\")\n",
        "\n",
        "    return W.cpu().numpy(), H.cpu().numpy(), perf.cpu()"
      ],
      "metadata": {
        "id": "7FkQeivjkDYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your implemented NMF-Poisson"
      ],
      "metadata": {
        "id": "uHV7WG4Apkts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_nmf_psn, H_nmf_psn, nmf_psn_perf = nmf_psn(X.T, K, niter=100)"
      ],
      "metadata": {
        "id": "VKTqUF9Vm4j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check your implementation with `monitor_perf`."
      ],
      "metadata": {
        "id": "lJHSfqF_ppyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monitor_perf(nmf_psn_perf, \"Poisson\")"
      ],
      "metadata": {
        "id": "Qwik7ENLndQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare NMF-SSE and NMF-Poisson\n",
        "Comparing the two models, we observe that the NMF-Poisson model led to higher ARI than the NMF-SSE model. This implies that the Poisson likelihood is a better objective function to model the discrete read counts of the scRNA-seq data than the SSE loss. The latter is equivalent to maximizing the log of Gaussian likelihood."
      ],
      "metadata": {
        "id": "PiTez6zSnAU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare NMF-SSE and NMF-Poisson\n",
        "fig, ax = plt.subplots()\n",
        "nmf_sse_perf_df = pd.DataFrame(data=nmf_sse_perf, columns=['Iter', \"SSE\", 'ARI'])\n",
        "nmf_psn_perf_df = pd.DataFrame(data=nmf_psn_perf, columns=['Iter', \"Poisson\", 'ARI'])\n",
        "ax.plot(nmf_sse_perf_df[\"Iter\"], nmf_sse_perf_df[\"ARI\"], color='blue', label='NMF-SSE')\n",
        "ax.plot(nmf_psn_perf_df[\"Iter\"], nmf_psn_perf_df[\"ARI\"], color='red', label='NMF-PSN')\n",
        "ax.legend()\n",
        "plt.xlabel(\"Iteration\");\n",
        "plt.ylabel(\"ARI\")"
      ],
      "metadata": {
        "id": "oego92mHm63F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Topic Model (ETM)\n",
        "The codeblock bleow implements a simpler strip-down version of the scETM (Zhao et al, 2021). The code is the same as the original ETM implementation by Dieng et al (2020) that is available from \\url{https://github.com/adjidieng/ETM}.\n"
      ],
      "metadata": {
        "id": "EWqRwyhOq2P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ETM implementation (DO NOT MODIFY)\n",
        "class ETM(nn.Module):\n",
        "    def __init__(self, num_topics, vocab_size, t_hidden_size, rho_size, emsize,\n",
        "                    theta_act, embeddings=None, train_embeddings=True, enc_drop=0.5):\n",
        "        super(ETM, self).__init__()\n",
        "\n",
        "        ## define hyperparameters\n",
        "        self.num_topics = num_topics\n",
        "        self.vocab_size = vocab_size\n",
        "        self.t_hidden_size = t_hidden_size\n",
        "        self.rho_size = rho_size\n",
        "        self.enc_drop = enc_drop\n",
        "        self.emsize = emsize\n",
        "        self.t_drop = nn.Dropout(enc_drop)\n",
        "\n",
        "        self.theta_act = self.get_activation(theta_act)\n",
        "\n",
        "        self.train_embeddings = train_embeddings\n",
        "\n",
        "        ## define the word embedding matrix \\rho\n",
        "        if self.train_embeddings:\n",
        "            self.rho = nn.Parameter(torch.randn(vocab_size, rho_size)) # V x L\n",
        "        else:\n",
        "            self.rho = embeddings.clone().float().to(device) # V x L\n",
        "\n",
        "        ## define the matrix containing the topic embeddings\n",
        "        self.alphas = nn.Linear(rho_size, num_topics, bias=False)#nn.Parameter(torch.randn(rho_size, num_topics))\n",
        "\n",
        "        ## define variational distribution for \\theta_{1:D} via amortizartion\n",
        "        self.q_theta = nn.Sequential(\n",
        "                nn.Linear(vocab_size, t_hidden_size),\n",
        "                self.theta_act,\n",
        "                nn.Linear(t_hidden_size, t_hidden_size),\n",
        "                self.theta_act,\n",
        "            )\n",
        "        self.mu_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
        "        self.logsigma_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
        "\n",
        "    def get_activation(self, act):\n",
        "        if act == 'tanh':\n",
        "            act = nn.Tanh()\n",
        "        elif act == 'relu':\n",
        "            act = nn.ReLU()\n",
        "        elif act == 'softplus':\n",
        "            act = nn.Softplus()\n",
        "        elif act == 'rrelu':\n",
        "            act = nn.RReLU()\n",
        "        elif act == 'leakyrelu':\n",
        "            act = nn.LeakyReLU()\n",
        "        elif act == 'elu':\n",
        "            act = nn.ELU()\n",
        "        elif act == 'selu':\n",
        "            act = nn.SELU()\n",
        "        elif act == 'glu':\n",
        "            act = nn.GLU()\n",
        "        else:\n",
        "            print('Defaulting to tanh activations...')\n",
        "            act = nn.Tanh()\n",
        "        return act\n",
        "\n",
        "    # theta ~ mu + std N(0,1)\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Returns a sample from a Gaussian distribution via reparameterization.\n",
        "        \"\"\"\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul_(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def encode(self, bows):\n",
        "        \"\"\"Returns paramters of the variational distribution for \\theta.\n",
        "\n",
        "        input: bows\n",
        "                batch of bag-of-words...tensor of shape bsz x V\n",
        "        output: mu_theta, log_sigma_theta\n",
        "        \"\"\"\n",
        "        q_theta = self.q_theta(bows)\n",
        "        if self.enc_drop > 0:\n",
        "            q_theta = self.t_drop(q_theta)\n",
        "        mu_theta = self.mu_q_theta(q_theta)\n",
        "        logsigma_theta = self.logsigma_q_theta(q_theta)\n",
        "\n",
        "        # KL[q(theta)||p(theta)] = lnq(theta) - lnp(theta)\n",
        "        kl_theta = -0.5 * torch.sum(1 + logsigma_theta - mu_theta.pow(2) - logsigma_theta.exp(), dim=-1).mean()\n",
        "        return mu_theta, logsigma_theta, kl_theta\n",
        "\n",
        "    def get_beta(self):\n",
        "\n",
        "        ## softmax over vocab dimension\n",
        "        beta = F.softmax(self.alphas(self.rho), dim=0).transpose(1, 0)\n",
        "        return beta\n",
        "\n",
        "    def get_theta(self, normalized_bows):\n",
        "        mu_theta, logsigma_theta, kld_theta = self.encode(normalized_bows)\n",
        "        z = self.reparameterize(mu_theta, logsigma_theta)\n",
        "        theta = F.softmax(z, dim=-1)\n",
        "        return theta, kld_theta\n",
        "\n",
        "    def decode(self, theta, beta):\n",
        "        res = torch.mm(theta, beta)\n",
        "        preds = torch.log(res+1e-6)\n",
        "        return preds\n",
        "\n",
        "    def forward(self, bows, normalized_bows, theta=None, aggregate=True):\n",
        "        ## get \\theta\n",
        "        if theta is None:\n",
        "            theta, kld_theta = self.get_theta(normalized_bows)\n",
        "        else:\n",
        "            kld_theta = None\n",
        "\n",
        "        ## get \\beta\n",
        "        beta = self.get_beta()\n",
        "\n",
        "        ## get prediction loss\n",
        "        preds = self.decode(theta, beta)\n",
        "        recon_loss = -(preds * bows).sum(1)\n",
        "        if aggregate:\n",
        "            recon_loss = recon_loss.mean()\n",
        "        return recon_loss, kld_theta\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Fyon-hlAsBUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code to instantiate an ETM model object is already written for you with pre-specified hyperparameters (i.e., topic number, hidden size, learning rates, weight decay penalty, etc). You want to start with these settings. Once you get the model working, you may play around with these parameters to get higher ARI. But that is not mandatory.\n"
      ],
      "metadata": {
        "id": "76QRpdrfszc0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Y7N7G0lVCS"
      },
      "outputs": [],
      "source": [
        "model = ETM(num_topics=K,\n",
        "            vocab_size=len(genes),\n",
        "            t_hidden_size=256,\n",
        "            rho_size=256,\n",
        "            emsize=256,\n",
        "            theta_act='relu',\n",
        "            embeddings=None,\n",
        "            train_embeddings=True,\n",
        "            enc_drop=0.5).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1.2e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 Implementing ETM\n",
        "Implement the wrapper function called `train_ETM` that uses the helper functions `train_ETM_helper` to train a ETM and another completed helper function called `get_theta` to compute cell embedding topic mixture $\\mathbf{\\theta}$. The latter two functions have been provided to you. **Do not modify them**.\n",
        "\n",
        "Note here that `get_theta` takes the normalized gene counts by the total count per cell stored in `X_tensor_normalized` as the input to the encoder neural network. The final reconstruction categorical likelihood is based on the unnormalized count data stored in `X_tensor`.\n",
        "\n",
        "PyTorch by default trace the error derivatives to calculate the gradient for backpropagation. When evaluating ARI, you want to turn off this automatic gradient trace by coding under the code block `with torch.no_grad()`.\n",
        "\n",
        "Here ARI is computed based on the Louvain clustering the cell embedding $\\mathbf{\\theta}$ against the ground-truth cell types stored in the `mp_anndata` object."
      ],
      "metadata": {
        "id": "Wl0xpI27ub-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9PHThZslVCS"
      },
      "outputs": [],
      "source": [
        "# train the VAE for one epoch\n",
        "def train_ETM_helper(model, X_tensor, X_tensor_normalized):\n",
        "    # initialize the model and loss\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    model.zero_grad()\n",
        "\n",
        "    # forward and backward pass\n",
        "    nll, kl_theta = model(X_tensor, X_tensor_normalized)\n",
        "    loss = nll + kl_theta\n",
        "    loss.backward()  # backprop gradients w.r.t. negative ELBO\n",
        "\n",
        "    # clip gradients to 2.0 if it gets too large\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "\n",
        "    # update model to minimize negative ELBO\n",
        "    optimizer.step()\n",
        "\n",
        "    return torch.sum(loss).item()\n",
        "\n",
        "# get sample encoding theta from the trained encoder network\n",
        "def get_theta(model, input_x):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        q_theta = model.q_theta(input_x)\n",
        "        mu_theta = model.mu_q_theta(q_theta)\n",
        "        theta = F.softmax(mu_theta, dim=-1)\n",
        "        return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqd2VKXAlVCT"
      },
      "outputs": [],
      "source": [
        "def train_ETM(model, X, adata=mp_anndata, niter=1000):\n",
        "    \"\"\"\n",
        "        :param model: the ETM model object\n",
        "        :param X: NxM raw read count matrix X\n",
        "        :param adata: annotated single-cell data object with ground-truth cell type information for evaluation\n",
        "        :param niter: maximum number of epochs\n",
        "        :return:\n",
        "            1. model: trained ETM model object\n",
        "            2. perf: niter-by-3 ndarray with iteration index, NELBO, and ARI as the 3 columns\n",
        "    \"\"\"\n",
        "    # Initialize W and H with random values, ensuring they're on the GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    X_tensor = torch.from_numpy(np.array(X, dtype=\"float32\"))\n",
        "    sums = X_tensor.sum(1).unsqueeze(1)\n",
        "    X_tensor_normalized = X_tensor / sums\n",
        "\n",
        "    # Ensure X, W, and H are on the correct device\n",
        "    X_tensor = X_tensor.to(device)\n",
        "    X_tensor_normalized = X_tensor_normalized.to(device)\n",
        "\n",
        "    # Initialize performance tracking array\n",
        "    perf = torch.zeros((niter, 3), dtype=torch.float32, device=device)\n",
        "\n",
        "    for i in range(niter):\n",
        "        nelbo = train_ETM_helper(model, X_tensor, X_tensor_normalized)\n",
        "        with torch.no_grad():\n",
        "            theta = get_theta(model, X_tensor_normalized)\n",
        "            perf[i, 0] = i\n",
        "            perf[i, 1] = nelbo\n",
        "            perf[i, 2] = evaluate_ari(theta.cpu().numpy(), adata)\n",
        "            print('Iter: {} ..  NELBO: {:.4f} .. ARI: {:.4f}'.format(i, perf[i, 1], perf[i, 2]))\n",
        "\n",
        "    return model, perf.cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training ETM requires many epochs because of the gradient descent updates with small learning rate. For debugging, start with 10 epochs. Once you confirm that your code is working, train ETM for 1000 epochs. Record the negative ELBO loss and ARI at each iteration and then use `monitor_perf` to display the training progress. With T4 enabled on Colab (or M1 Max on Mac), 1000 epochs take less than 5 minutes. CPU takes about 10 times longer."
      ],
      "metadata": {
        "id": "iJmmEBIqwDj4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq5KYf2FlVCT"
      },
      "outputs": [],
      "source": [
        "model, scetm_perf = train_ETM(model, X, niter=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the training performance"
      ],
      "metadata": {
        "id": "ZCRsglp-x99u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w68bxZdOlVCU"
      },
      "outputs": [],
      "source": [
        "monitor_perf(scetm_perf, \"NELBO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 compare ETM with NMF-Poisson\n",
        "Now run NMF-Poisson also for 1000 iterations and compare the ARI scores with scETM over the 1000 iterations. While the NMF model converges to a local optimal after only 200 iterations, the ETM continues to improve. We observe some improvement from ETM over the NMF model especially after 200 iterations. This highlights the benefits of having the non-linear encoder function and perhaps the tri-factorization design in the ETM. When training on massive number of single-cell samples, using SGD on minibatches, ETM may confer bigger improvement. For this particular dataset, with batch-effect correction, a fine-tuned ETM can reach 0.90 ARI (Zhao et al., 2021)."
      ],
      "metadata": {
        "id": "hML6QTZJIlNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_nmf_psn, H_nmf_psn, nmf_psn_perf = nmf_psn(X.T, K, niter=1000)\n",
        "_, ax = plt.subplots()\n",
        "nmf_psn_perf_df = pd.DataFrame(data=nmf_psn_perf, columns=['Iter', \"Poisson\", 'ARI'])\n",
        "scetm_perf_df = pd.DataFrame(data=scetm_perf, columns=['Iter', \"NELBO\", 'ARI'])\n",
        "ax.plot(nmf_psn_perf_df[\"Iter\"], nmf_psn_perf_df[\"ARI\"], color='red', label='NMF-PSN')\n",
        "ax.plot(scetm_perf_df[\"Iter\"], scetm_perf_df[\"ARI\"], color='black', label='ETM')\n",
        "plt.xlabel(\"Iteration\"); plt.ylabel(\"ARI\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IiVJ5j6sIf8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lN8630IlVCU"
      },
      "source": [
        "# Task 5 Generate t-SNE to visualize cell embeddings\n",
        "Use the `model` object of ETM saved from the previous training over the 1000 iterations to infer final cell topic embedding $\\mathbf{\\theta}$ and generate the two-dimensional t-SNE plot. Also, use the `H` matrix from the NMF-Poisson model (trained after 1000 iterations) to generate another two-dimensional t-SNE plot. Compare them side-by-side as shown in plots below, respectively. We observe a slightly better separation of the alpha cells from other cells from the ETM compared to the NMF-Poisson model.\n",
        "\n",
        "The plot was generated using a `Scanpy` function called `sc.tl.tsne`. Learn how to use it from its documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKaDM6TKlVCU"
      },
      "outputs": [],
      "source": [
        "def get_tsne(cell_embed, adata, mytitle):\n",
        "    ari = evaluate_ari(cell_embed, adata)\n",
        "    sc.tl.tsne(adata, use_rep='cell_embed', n_pcs=10, use_fast_tsne=False)\n",
        "    _, ax = plt.subplots(figsize=(7, 6))\n",
        "    sc.pl.tsne(adata, color=[\"Celltype\"],\n",
        "               ax=ax, title=mytitle + ' (ARI: {:.2f})'.format(round(ari, 2)))\n",
        "\n",
        "get_tsne(get_theta(model, X_tensor_normalized).cpu().numpy(), mp_anndata, 'ETM on MP')\n",
        "get_tsne(H_nmf_psn.T, mp_anndata, 'NMF-Poisson on MP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp4tB3xulVCV"
      },
      "source": [
        "# Task 6 Plot heatmap for the cells under each topic\n",
        "An alternative way to present the cell cluster is by heatmap. Heatmap is more effective in identifying which topics correlates well with which cell types. Generate a heatmap plot for the same cells-by-topics matrix $\\mathbf{\\theta}$ using *all of the 1886 cells over the 16 topics*. Your heatmap should look similar to the plot below, which was generated using `seaborn.clustermap`.\n",
        "\n",
        "From here, we see that topic 7 correlates well with alpha cell type, topic 2 with endothelial, topic 6 with ductal cell type, and so forth. Note that you will get different topic indices matching with different cell types as the order of these topics are not the same at different runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKgiHjoelVCV"
      },
      "outputs": [],
      "source": [
        "theta = get_theta(model, X_tensor_normalized)\n",
        "theta_df = pd.DataFrame(theta.cpu().detach().numpy())\n",
        "\n",
        "cell_types = pd.Series(list(mp_anndata.obs['Celltype']))\n",
        "celltype_list = list(pd.unique(list(mp_anndata.obs['Celltype'])))\n",
        "celltype_list.sort()\n",
        "color_list = list(mp_anndata.uns['Celltype_colors'])\n",
        "lut = dict(zip(celltype_list, color_list))\n",
        "cell_type_colors = cell_types.map(lut)\n",
        "\n",
        "g = clustermap(theta_df, center=0, cmap=\"RdBu_r\", row_colors=cell_type_colors)\n",
        "\n",
        "handles = [Patch(facecolor=lut[name]) for name in lut]\n",
        "plt.legend(handles, lut, title='Cell types',\n",
        "           bbox_to_anchor=(1, 1),\n",
        "           bbox_transform=plt.gcf().transFigure,\n",
        "           loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyNy83dmlVCV"
      },
      "source": [
        "# Task 7 Plot heatmap for the top genes under each topic\n",
        "To get cell-type-specific gene signature, we can visualize the genes by topics heatmap. Here we will plot the top 5 genes per topic in heatmap. Your heatmap should look similar to one below, which was generated using `seaborn.heatmap`. In plotting the heatmap, I capped the max value at 0.2 instead of letting it set to 1 to make the red intensities more prominent for some of the genes with low absolute value under some of the topics.\n",
        "\n",
        "What cell-type-specific gene signatures can you find in your analysis? For example, from the heatmap we know that topic 7 corresponds to alpha cell type. The gene *Gcg*, which codes for protein Glucagon has the highest probability under that topic. Based on https://www.proteinatlas.org/ENSG00000115263-GCG, Glucagon is indeed a pancreatic hormone that counteracts the glucose-lowering action of insulin by stimulating glycogenolysis and gluconeogenesis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aDlwOr4lVCV"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "beta = model.get_beta()\n",
        "beta_df = pd.DataFrame(beta.t().cpu().detach().numpy())\n",
        "beta_df.index = genes\n",
        "topN = 5\n",
        "top_genes_topics = [beta_df.loc[beta_df[k].sort_values(ascending=False).head(topN).index] for k in range(K)]\n",
        "top_genes_topics = pd.concat(top_genes_topics)\n",
        "\n",
        "_, ax = plt.subplots(figsize=(6, 16))\n",
        "g = heatmap(top_genes_topics, center=0,\n",
        "            cmap=\"RdBu_r\", vmax=0.2,\n",
        "            linewidths=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7BL0DyplVCV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}